\chapter{Extensibility}

\section{Modules}

The concept of modules enables to easily integrate additional functionality. By forcing new classes to inherit from module, information that is common over all modules can be easily added, for example an internal version information. The integrated module categories (Solver, Policy,...) provide interfaces in order to avoid any changes in the existing code base. 

\section{Partial Observability}

One variant of MDPs are partial observable Markov decision processes, abbreviated by \emph{POMDP}. In addition to a MDP, the state cannot be directly observed. Instead \emph{observations} are accessible which can be assigned to a certain state with some probability. By inheriting from the Model class a POMDP class can be easily integrated. 

\section{Reinforcement Learning}

Reinforcement learning problems can also be defined as a MDP but with (at least partially) unknown state transition behavior. A common approach is to first learn the state transition behavior by drawing samples from an environment and then solving the defined MDP. Reinforcement learning algorithms can be added to the Solver module category. A new module category with different environment implementations can manage sample data that is generated by rolling out a policy on the defined environment. This extension is compatible to the current architecture.  